{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install numpy\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize and assign the data to classes to load in batches for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Quick normalization first\n",
    "def quick_normalize(game_objects):\n",
    "    print(\"Starting quick normalization...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Collect all data to fit the scaler\n",
    "    print(\"Collecting all data to fit scaler...\")\n",
    "    all_data = []\n",
    "    for game in game_objects:\n",
    "        all_data.append(game.matchups)\n",
    "        all_data.append(game.team_history)\n",
    "        all_data.append(game.opponent_history)\n",
    "    \n",
    "    # Concatenate all data and fit the scaler\n",
    "    print(\"Fitting scaler...\")\n",
    "    combined_data = pd.concat(all_data)\n",
    "    scaler.fit(combined_data)\n",
    "    \n",
    "    # Now transform each game's data using the fitted scaler\n",
    "    print(\"Transforming data...\")\n",
    "    for i, game in enumerate(game_objects):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Normalizing game {i}/{len(game_objects)}\")\n",
    "            \n",
    "        # Normalize game_date\n",
    "        baseline_date = pd.to_datetime(\"2000-01-01\")\n",
    "        game.game_date = (pd.to_datetime(game.game_date) - baseline_date).days\n",
    "        \n",
    "        # Transform each sequence using the same scaler\n",
    "        game.matchups = pd.DataFrame(\n",
    "            scaler.transform(game.matchups),\n",
    "            columns=game.matchups.columns,\n",
    "            index=game.matchups.index\n",
    "        )\n",
    "        game.team_history = pd.DataFrame(\n",
    "            scaler.transform(game.team_history),\n",
    "            columns=game.team_history.columns,\n",
    "            index=game.team_history.index\n",
    "        )\n",
    "        game.opponent_history = pd.DataFrame(\n",
    "            scaler.transform(game.opponent_history),\n",
    "            columns=game.opponent_history.columns,\n",
    "            index=game.opponent_history.index\n",
    "        )\n",
    "    \n",
    "    return game_objects, scaler\n",
    "\n",
    "# Classes for PyTorch handling\n",
    "class GameData:\n",
    "    def __init__(self, game_id, game_date, is_regular_season, is_playoffs, \n",
    "                 is_pre_season, matchups, team_history, opponent_history, target):\n",
    "        self.game_id = game_id\n",
    "        self.game_date = game_date\n",
    "        self.is_regular_season = is_regular_season\n",
    "        self.is_playoffs = is_playoffs\n",
    "        self.is_pre_season = is_pre_season\n",
    "        self.matchups = matchups\n",
    "        self.team_history = team_history\n",
    "        self.opponent_history = opponent_history\n",
    "        self.target = target\n",
    "\n",
    "    def to_tensor_dict(self):\n",
    "        tensor_dict = {\n",
    "            'matchups_tensor': torch.FloatTensor(self.matchups.values),\n",
    "            'team_history_tensor': torch.FloatTensor(self.team_history.values),\n",
    "            'opponent_history_tensor': torch.FloatTensor(self.opponent_history.values),\n",
    "            'matchups_lengths': torch.LongTensor([len(self.matchups)]),\n",
    "            'team_history_lengths': torch.LongTensor([len(self.team_history)]),\n",
    "            'opponent_history_lengths': torch.LongTensor([len(self.opponent_history)]),\n",
    "            'target': torch.FloatTensor([self.target]),\n",
    "            'game_type': torch.FloatTensor([\n",
    "                self.is_regular_season,\n",
    "                self.is_playoffs,\n",
    "                self.is_pre_season\n",
    "            ])\n",
    "        }\n",
    "        return tensor_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_batch(batch):\n",
    "        tensor_dicts = [game.to_tensor_dict() for game in batch]\n",
    "        batch_dict = {}\n",
    "        \n",
    "        # No need for lengths or padding since sequences are fixed length\n",
    "        for key in ['matchups_tensor', 'team_history_tensor', 'opponent_history_tensor']:\n",
    "            batch_dict[key] = torch.stack([d[key] for d in tensor_dicts])\n",
    "        \n",
    "        batch_dict['game_type'] = torch.stack([d['game_type'] for d in tensor_dicts])\n",
    "        batch_dict['target'] = torch.cat([d['target'] for d in tensor_dicts])\n",
    "        \n",
    "        return batch_dict\n",
    "\n",
    "class NBAGamesDataset(Dataset):\n",
    "    def __init__(self, game_objects):\n",
    "        self.game_objects = game_objects\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.game_objects)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.game_objects[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data and save to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting quick normalization...\n",
      "Collecting all data to fit scaler...\n",
      "Fitting scaler...\n",
      "Transforming data...\n",
      "Normalizing game 0/28803\n",
      "Normalizing game 1000/28803\n",
      "Normalizing game 2000/28803\n",
      "Normalizing game 3000/28803\n",
      "Normalizing game 4000/28803\n",
      "Normalizing game 5000/28803\n",
      "Normalizing game 6000/28803\n",
      "Normalizing game 7000/28803\n",
      "Normalizing game 8000/28803\n",
      "Normalizing game 9000/28803\n",
      "Normalizing game 10000/28803\n",
      "Normalizing game 11000/28803\n",
      "Normalizing game 12000/28803\n",
      "Normalizing game 13000/28803\n",
      "Normalizing game 14000/28803\n",
      "Normalizing game 15000/28803\n",
      "Normalizing game 16000/28803\n",
      "Normalizing game 17000/28803\n",
      "Normalizing game 18000/28803\n",
      "Normalizing game 19000/28803\n",
      "Normalizing game 20000/28803\n",
      "Normalizing game 21000/28803\n",
      "Normalizing game 22000/28803\n",
      "Normalizing game 23000/28803\n",
      "Normalizing game 24000/28803\n",
      "Normalizing game 25000/28803\n",
      "Normalizing game 26000/28803\n",
      "Normalizing game 27000/28803\n",
      "Normalizing game 28000/28803\n",
      "\n",
      "Number of training batches: 721\n",
      "Number of validation batches: 90\n",
      "Number of test batches: 91\n"
     ]
    }
   ],
   "source": [
    "with open('rnn_game_objects.pkl', 'rb') as f:\n",
    "    game_objects = pickle.load(f)\n",
    "\n",
    "# Quick normalize\n",
    "normalized_games, scaler = quick_normalize(game_objects)\n",
    "\n",
    "# Split into train/val/test\n",
    "game_dates = np.array([game.game_date for game in normalized_games])\n",
    "sorted_indices = np.argsort(game_dates)\n",
    "normalized_games = [normalized_games[i] for i in sorted_indices]\n",
    "\n",
    "n_train = int(0.8 * len(normalized_games))\n",
    "n_val = int(0.10 * len(normalized_games))\n",
    "\n",
    "train_games = normalized_games[:n_train]\n",
    "val_games = normalized_games[n_train:n_train + n_val]\n",
    "test_games = normalized_games[n_train + n_val:]\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "batch_size = 32\n",
    "train_dataset = NBAGamesDataset(train_games)\n",
    "val_dataset = NBAGamesDataset(val_games)\n",
    "test_dataset = NBAGamesDataset(test_games)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=GameData.collate_batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=GameData.collate_batch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=GameData.collate_batch\n",
    ")\n",
    "\n",
    "# Save everything\n",
    "with open('game_stats_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "processed_data = {\n",
    "    'train_dataset': train_loader.dataset.game_objects,\n",
    "    'val_dataset': val_loader.dataset.game_objects,\n",
    "    'test_dataset': test_loader.dataset.game_objects\n",
    "}\n",
    "\n",
    "with open('processed_game_objects.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "# Print information\n",
    "print(f\"\\nNumber of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PredictorRNN(nn.Module):\n",
    "    def __init__(self, feature_names, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.feature_names = feature_names\n",
    "        self.input_size = len(feature_names)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM networks for each sequence type\n",
    "        self.matchups_lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        self.team_history_lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        self.opponent_history_lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Game type embedding\n",
    "        self.game_type_proj = nn.Linear(3, hidden_size)\n",
    "        \n",
    "        # Combination layer\n",
    "        combined_size = (hidden_size * 3) + hidden_size  # 3 LSTMs + game type\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_size, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, batch_dict):\n",
    "        # Process matchups (always length 5)\n",
    "        _, (matchups_hidden, _) = self.matchups_lstm(batch_dict['matchups_tensor'])\n",
    "        matchups_hidden = matchups_hidden[-1]  # Take last layer's hidden state\n",
    "        \n",
    "        # Process team history (always length 10)\n",
    "        _, (team_hidden, _) = self.team_history_lstm(batch_dict['team_history_tensor'])\n",
    "        team_hidden = team_hidden[-1]\n",
    "        \n",
    "        # Process opponent history (always length 10)\n",
    "        _, (opponent_hidden, _) = self.opponent_history_lstm(batch_dict['opponent_history_tensor'])\n",
    "        opponent_hidden = opponent_hidden[-1]\n",
    "        \n",
    "        # Process game type\n",
    "        game_type_embedded = self.game_type_proj(batch_dict['game_type'])\n",
    "        \n",
    "        # Combine all features\n",
    "        combined = torch.cat([\n",
    "            matchups_hidden,\n",
    "            team_hidden,\n",
    "            opponent_hidden,\n",
    "            game_type_embedded\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Make prediction and ensure output is between 0 and 1\n",
    "        output = self.classifier(combined)\n",
    "        output = torch.clamp(output, 0, 1)  # Add this line\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def train_model(self, train_loader, val_loader, num_epochs=50, learning_rate=0.001, patience=5):\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            self.train()\n",
    "            total_train_loss = 0\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch)\n",
    "                # Reshape target to match output shape\n",
    "                target = batch['target'].view(-1, 1)  # Change this line\n",
    "                loss = criterion(outputs, target)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "                predictions = (outputs >= 0.5).float()\n",
    "                correct_predictions += (predictions == target).sum().item()  # Update this line\n",
    "                total_predictions += target.size(0)\n",
    "            \n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            train_accuracy = correct_predictions / total_predictions\n",
    "            \n",
    "            # Validation phase\n",
    "            self.eval()\n",
    "            total_val_loss = 0\n",
    "            correct_val_predictions = 0\n",
    "            total_val_predictions = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    outputs = self(batch)\n",
    "                    target = batch['target'].view(-1, 1)  # Add this line\n",
    "                    loss = criterion(outputs, target)\n",
    "                    \n",
    "                    total_val_loss += loss.item()\n",
    "                    predictions = (outputs >= 0.5).float()\n",
    "                    correct_val_predictions += (predictions == target).sum().item()  # Update this line\n",
    "                    total_val_predictions += target.size(0)\n",
    "            \n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            val_accuracy = correct_val_predictions / total_val_predictions\n",
    "            \n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model = self.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "            print(f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "            print(f'Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "            print(f'Patience Counter: {patience_counter}/{patience}')\n",
    "            print('--------------------')\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "        self.load_state_dict(best_model)\n",
    "        return self, {\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'epochs_trained': epoch + 1\n",
    "        }\n",
    "\n",
    "def create_feature_filtered_loader(original_loader, feature_names, batch_size):\n",
    "    filtered_games = []\n",
    "\n",
    "    for game in original_loader.dataset.game_objects:\n",
    "        # Check for NaN values in all sequences\n",
    "        has_nan = (game.matchups[feature_names].isna().any().any() or \n",
    "                  game.team_history[feature_names].isna().any().any() or \n",
    "                  game.opponent_history[feature_names].isna().any().any())\n",
    "        \n",
    "        if not has_nan:  # Only include games without NaN values\n",
    "            filtered_game = GameData(\n",
    "                game_id=game.game_id,\n",
    "                game_date=game.game_date,\n",
    "                is_regular_season=game.is_regular_season,\n",
    "                is_playoffs=game.is_playoffs,\n",
    "                is_pre_season=game.is_pre_season,\n",
    "                matchups=game.matchups[feature_names],\n",
    "                team_history=game.team_history[feature_names],\n",
    "                opponent_history=game.opponent_history[feature_names],\n",
    "                target=game.target\n",
    "            )\n",
    "            filtered_games.append(filtered_game)\n",
    "    \n",
    "    print(f\"Filtered out {len(original_loader.dataset.game_objects) - len(filtered_games)} games with NaN values\")\n",
    "    print(f\"Remaining games: {len(filtered_games)}\")\n",
    "\n",
    "    filtered_dataset = NBAGamesDataset(filtered_games)\n",
    "\n",
    "    return DataLoader(\n",
    "        filtered_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True if original_loader.dataset == 'train' else False,\n",
    "        collate_fn=GameData.collate_batch\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and check accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 10 games with NaN values\n",
      "Remaining games: 23032\n",
      "Filtered out 0 games with NaN values\n",
      "Remaining games: 2880\n",
      "Epoch 1/50\n",
      "Train Loss: 0.6619, Train Accuracy: 0.6104\n",
      "Val Loss: 0.6780, Val Accuracy: 0.5740\n",
      "Patience Counter: 0/5\n",
      "--------------------\n",
      "Epoch 2/50\n",
      "Train Loss: 0.6553, Train Accuracy: 0.6151\n",
      "Val Loss: 0.6799, Val Accuracy: 0.5694\n",
      "Patience Counter: 1/5\n",
      "--------------------\n",
      "Epoch 3/50\n",
      "Train Loss: 0.6545, Train Accuracy: 0.6187\n",
      "Val Loss: 0.6819, Val Accuracy: 0.5608\n",
      "Patience Counter: 2/5\n",
      "--------------------\n",
      "Epoch 4/50\n",
      "Train Loss: 0.6542, Train Accuracy: 0.6169\n",
      "Val Loss: 0.6807, Val Accuracy: 0.5601\n",
      "Patience Counter: 3/5\n",
      "--------------------\n",
      "Epoch 5/50\n",
      "Train Loss: 0.6534, Train Accuracy: 0.6195\n",
      "Val Loss: 0.6806, Val Accuracy: 0.5576\n",
      "Patience Counter: 4/5\n",
      "--------------------\n",
      "Epoch 6/50\n",
      "Train Loss: 0.6529, Train Accuracy: 0.6181\n",
      "Val Loss: 0.6833, Val Accuracy: 0.5583\n",
      "Patience Counter: 5/5\n",
      "--------------------\n",
      "Early stopping triggered after epoch 6\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('processed_game_objects.pkl', 'rb') as f:\n",
    "    processed_data = pickle.load(f)\n",
    "\n",
    "features = [\n",
    "    'days_ago', \n",
    "    'games_ago', \n",
    "    'is_home_team', \n",
    "    'is_regular_season_matchup', \n",
    "    'is_playoffs_matchup', \n",
    "    'is_pre_season_matchup', \n",
    "    'wl', \n",
    "    'pts_for', \n",
    "    'fg_pct_for', \n",
    "    'fg3_pct_for', \n",
    "    'fg3m_for', \n",
    "    'ft_pct_for', \n",
    "    'ftm_for', \n",
    "    'reb_for', \n",
    "    'ast_for', \n",
    "    'stl_for', \n",
    "    'blk_for', \n",
    "    'tov_for', \n",
    "    'pts_against', \n",
    "    'fg_pct_against', \n",
    "    'fg3_pct_against', \n",
    "    'fg3m_against', \n",
    "    'ft_pct_against', \n",
    "    'ftm_against', \n",
    "    'reb_against', \n",
    "    'ast_against', \n",
    "    'stl_against', \n",
    "    'blk_against', \n",
    "    'tov_against'\n",
    "    ]\n",
    "\n",
    "train_loader = create_feature_filtered_loader(train_loader, features, batch_size=32)\n",
    "val_loader = create_feature_filtered_loader(val_loader, features, batch_size=32)\n",
    "\n",
    "# Create and train model\n",
    "model = PredictorRNN(feature_names=features)\n",
    "trained_model, history = model.train_model(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 0 games with NaN values\n",
      "Remaining games: 2881\n",
      "\n",
      "Test Results:\n",
      "Total Test Games: 2881\n",
      "Correct Predictions: 1657\n",
      "Test Accuracy: 0.5751\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for batch in test_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(batch)\n",
    "            target = batch['target'].view(-1, 1)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = (outputs >= 0.5).float()\n",
    "            \n",
    "            # Track accuracy\n",
    "            correct_predictions += (predictions == target).sum().item()\n",
    "            total_predictions += target.size(0)\n",
    "            \n",
    "            # Store predictions and targets for additional metrics\n",
    "            all_predictions.extend(predictions.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy().flatten())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Total Test Games: {total_predictions}\")\n",
    "    print(f\"Correct Predictions: {correct_predictions}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracy, all_predictions, all_targets\n",
    "\n",
    "# Usage:\n",
    "test_loader = create_feature_filtered_loader(test_loader, features, batch_size=32)\n",
    "accuracy, predictions, targets = evaluate_model(trained_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
