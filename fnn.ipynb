{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/a8/44/d9502bf0ed197ba9bf1103c9867d5904ddcaf869e52329787fc54ed70cc8/pandas-2.2.3-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading pandas-2.2.3-cp311-cp311-macosx_10_9_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.23.2 (from pandas)\n",
      "  Obtaining dependency information for numpy>=1.23.2 from https://files.pythonhosted.org/packages/e9/b5/306ac6ee3f8f0c51abd3664ee8a9b8e264cbf179a860674827151ecc0a9c/numpy-2.2.0-cp311-cp311-macosx_14_0_x86_64.whl.metadata\n",
      "  Downloading numpy-2.2.0-cp311-cp311-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/alidaeihagh/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/11/c3/005fcca25ce078d2cc29fd559379817424e94885510568bc1bc53d7d5846/pytz-2024.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/a6/ab/7e5f53c3b9d14972843a647d8d7a853969a58aecc7559cb3267302c94774/tzdata-2024.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alidaeihagh/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-macosx_10_9_x86_64.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.0-cp311-cp311-macosx_14_0_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.0 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/ff/91/609961972f694cb9520c4c3d201e377a26583e1eb83bc5a334c893729214/scikit_learn-1.5.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.6.0 from https://files.pythonhosted.org/packages/5e/fc/9f1413bef53171f379d786aabc104d4abeea48ee84c553a3e3d8c9f96a9c/scipy-1.14.1-cp311-cp311-macosx_14_0_x86_64.whl.metadata\n",
      "  Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.2.0 from https://files.pythonhosted.org/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp311-cp311-macosx_14_0_x86_64.whl (25.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.5/25.5 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/3f/14/e105b8ef6d324e789c1589e95cb0ab63f3e07c2216d68b1178b7c21b7d2a/torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/b9/f8/feced7779d755758a52d1f6635d990b8d98dc0a29fa568bbe0625f18fdf3/filelock-3.16.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alidaeihagh/Library/Python/3.11/lib/python/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Obtaining dependency information for sympy from https://files.pythonhosted.org/packages/99/ff/c87e0622b1dadea79d2fb0b25ade9ed98954c9033722eb707053d310d4f3/sympy-1.13.3-py3-none-any.whl.metadata\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/b9/54/dd730b32ea14ea797530a4479b2ed46a6fb250f682a9cfb997e968bf0261/networkx-3.4.2-py3-none-any.whl.metadata\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Obtaining dependency information for jinja2 from https://files.pythonhosted.org/packages/31/80/3a54838c3fb461f6fec263ebf3a3a41771bd05190238de3486aae8540c36/jinja2-3.1.4-py3-none-any.whl.metadata\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/c6/b2/454d6e7f0158951d8a78c2e1eb4f69ae81beb8dca5fee9809c6c99e9d0d0/fsspec-2024.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/6b/28/bbf83e3f76936960b850435576dd5e67034e200469571be53f69174a2dfd/MarkupSafe-3.0.2-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (4.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Obtaining dependency information for mpmath<1.4,>=1.1.0 from https://files.pythonhosted.org/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl.metadata\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-macosx_10_9_universal2.whl (14 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.10.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.3 torch-2.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install torch\n",
    "\n",
    "labels = [\n",
    "    'game_id',\n",
    "    'team_id_home',\n",
    "    'team_id_away',\n",
    "    'game_date',\n",
    "    'team_name_home',\n",
    "    'team_name_away',\n",
    "    'season_type'\n",
    "]\n",
    "features = [\n",
    "    # 'is_regular_season',\n",
    "    # 'is_playoffs',\n",
    "    # 'is_pre_season',\n",
    "    # 'days_ago',\n",
    "    # 'games_ago',\n",
    "    # 'is_home_game_matchup',\n",
    "    # 'is_regular_season_matchup',\n",
    "    # 'is_playoffs_matchup',\n",
    "    # 'is_pre_season_matchup',\n",
    "    # 'pts',\n",
    "    # 'opponent_pts',\n",
    "    'wl',\n",
    "    'pts_for',\n",
    "    # 'fg_pct_for',\n",
    "    # 'fg3_pct_for',\n",
    "    # 'fg3m_for',\n",
    "    # 'ft_pct_for',\n",
    "    # 'ftm_for',\n",
    "    # 'reb_for',\n",
    "    # 'ast_for',\n",
    "    # 'stl_for',\n",
    "    # 'blk_for',\n",
    "    # 'tov_for',\n",
    "    'pts_against',\n",
    "    # 'fg_pct_against',\n",
    "    # 'fg3_pct_against',\n",
    "    # 'fg3m_against',\n",
    "    # 'ft_pct_against',\n",
    "    # 'ftm_against',\n",
    "    # 'reb_against',\n",
    "    # 'ast_against',\n",
    "    # 'stl_against',\n",
    "    # 'blk_against',\n",
    "    # 'tov_against',\n",
    "    # 'team_hist_days_ago',\n",
    "    # 'team_hist_games_ago',\n",
    "    # 'team_hist_is_home_game_team_recent',\n",
    "    # 'team_hist_is_regular_season_team_recent',\n",
    "    # 'team_hist_is_playoffs_team_recent',\n",
    "    # 'team_hist_is_pre_season_team_recent',\n",
    "    # 'team_hist_pts',\n",
    "    # 'team_hist_opponent_pts',\n",
    "    'team_hist_wl',\n",
    "    'team_hist_pts_for',\n",
    "    # 'team_hist_fg_pct_for',\n",
    "    # 'team_hist_fg3_pct_for',\n",
    "    # 'team_hist_fg3m_for',\n",
    "    # 'team_hist_ft_pct_for',\n",
    "    # 'team_hist_ftm_for',\n",
    "    # 'team_hist_reb_for',\n",
    "    # 'team_hist_ast_for',\n",
    "    # 'team_hist_stl_for',\n",
    "    # 'team_hist_blk_for',\n",
    "    # 'team_hist_tov_for',\n",
    "    'team_hist_pts_against',\n",
    "    # 'team_hist_fg_pct_against',\n",
    "    # 'team_hist_fg3_pct_against',\n",
    "    # 'team_hist_fg3m_against',\n",
    "    # 'team_hist_ft_pct_against',\n",
    "    # 'team_hist_ftm_against',\n",
    "    # 'team_hist_reb_against',\n",
    "    # 'team_hist_ast_against',\n",
    "    # 'team_hist_stl_against',\n",
    "    # 'team_hist_blk_against',\n",
    "    # 'team_hist_tov_against'\n",
    "]\n",
    "\n",
    "\n",
    "# target=['pts_home','pts_away','wl_home']\n",
    "target=['wl_home']\n",
    "\n",
    "file_path = 'flat_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    wl  pts_for  pts_against  team_hist_wl  team_hist_pts_for  \\\n",
      "0  0.7    101.1         95.3           0.4               93.4   \n",
      "\n",
      "   team_hist_pts_against  \n",
      "0                   98.6  \n",
      "Training data shape: (24282, 6) (24282,)\n",
      "Testing data shape: (4608, 6) (4608,)\n",
      "Sample of training features:\n",
      "         wl     pts_for  pts_against  team_hist_wl  team_hist_pts_for  \\\n",
      "0  0.700000  101.100000    95.300000           0.4               93.4   \n",
      "1  0.571429   93.571429    94.285714           0.8              101.7   \n",
      "2  0.777778  101.111111    96.888889           0.2               89.0   \n",
      "3  0.700000   95.000000    94.400000           0.5               94.6   \n",
      "4  0.600000   94.300000    93.200000           0.6               84.7   \n",
      "\n",
      "   team_hist_pts_against  \n",
      "0                   98.6  \n",
      "1                   93.7  \n",
      "2                  100.0  \n",
      "3                   94.4  \n",
      "4                   85.1  \n",
      "\n",
      "Shape: (24282, 6)\n",
      "\n",
      "Feature information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24282 entries, 0 to 24281\n",
      "Data columns (total 6 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   wl                     24282 non-null  float64\n",
      " 1   pts_for                24282 non-null  float64\n",
      " 2   pts_against            24282 non-null  float64\n",
      " 3   team_hist_wl           24282 non-null  float64\n",
      " 4   team_hist_pts_for      24282 non-null  float64\n",
      " 5   team_hist_pts_against  24282 non-null  float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 1.3 MB\n",
      "None\n",
      "\n",
      "Feature statistics:\n",
      "                 wl       pts_for   pts_against  team_hist_wl  \\\n",
      "count  24282.000000  24282.000000  24282.000000  24282.000000   \n",
      "mean       0.496640     98.574973     98.689705      0.499604   \n",
      "std        0.218919      6.695258      6.706080      0.209168   \n",
      "min        0.000000     75.100000     74.700000      0.000000   \n",
      "25%        0.300000     94.100000     94.100000      0.375000   \n",
      "50%        0.500000     98.300000     98.444444      0.500000   \n",
      "75%        0.666667    102.900000    103.000000      0.666667   \n",
      "max        1.000000    125.900000    126.200000      1.000000   \n",
      "\n",
      "       team_hist_pts_for  team_hist_pts_against  \n",
      "count       24282.000000           24282.000000  \n",
      "mean           99.795947              99.809262  \n",
      "std             7.282704               7.266039  \n",
      "min            73.000000              69.000000  \n",
      "25%            94.800000              94.800000  \n",
      "50%            99.300000              99.500000  \n",
      "75%           104.500000             104.500000  \n",
      "max           129.100000             134.500000  \n",
      "\n",
      "Sample of target values (wl_home):\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: wl_home, dtype: int64\n",
      "\n",
      "Unique values in target: [1 0]\n",
      "Value counts in target:\n",
      " wl_home\n",
      "1    14496\n",
      "0     9786\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "file_path = 'flat_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure game_date is in datetime format\n",
    "data['game_date'] = pd.to_datetime(data['game_date'])\n",
    "\n",
    "# Split the data into train and test based on game_date\n",
    "train_data = data[(data['game_date'].dt.year >= 2001) & (data['game_date'].dt.year <= 2019)]\n",
    "test_data = data[(data['game_date'].dt.year >= 2020) & (data['game_date'].dt.year <= 2023)]\n",
    "\n",
    "# Extract features and target\n",
    "X_train = train_data[features]\n",
    "X_test = test_data[features]\n",
    "y_train = train_data['wl_home']\n",
    "y_test = test_data['wl_home']\n",
    "\n",
    "# print first row and num of features\n",
    "print(X_train.head(1))\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for convenience\n",
    "X_train_normalized = pd.DataFrame(X_train_normalized, columns=features)\n",
    "X_test_normalized = pd.DataFrame(X_test_normalized, columns=features)\n",
    "\n",
    "# Verify results\n",
    "print(\"Training data shape:\", X_train_normalized.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", X_test_normalized.shape, y_test.shape)\n",
    "\n",
    "# Look at first few rows of training features\n",
    "print(\"Sample of training features:\")\n",
    "print(X_train.head())\n",
    "print(\"\\nShape:\", X_train.shape)\n",
    "\n",
    "# Look at feature names and their data types\n",
    "print(\"\\nFeature information:\")\n",
    "print(X_train.info())\n",
    "\n",
    "# Get basic statistics of each feature\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(X_train.describe())\n",
    "\n",
    "# Look at some target values (wl_home)\n",
    "print(\"\\nSample of target values (wl_home):\")\n",
    "print(y_train.head())\n",
    "print(\"\\nUnique values in target:\", y_train.unique())\n",
    "print(\"Value counts in target:\\n\", y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6757\n",
      "Epoch 2/20, Loss: 0.6517\n",
      "Epoch 3/20, Loss: 0.6438\n",
      "Epoch 4/20, Loss: 0.6423\n",
      "Epoch 5/20, Loss: 0.6419\n",
      "Epoch 6/20, Loss: 0.6417\n",
      "Epoch 7/20, Loss: 0.6417\n",
      "Epoch 8/20, Loss: 0.6416\n",
      "Epoch 9/20, Loss: 0.6414\n",
      "Epoch 10/20, Loss: 0.6415\n",
      "Epoch 11/20, Loss: 0.6413\n",
      "Epoch 12/20, Loss: 0.6414\n",
      "Epoch 13/20, Loss: 0.6412\n",
      "Epoch 14/20, Loss: 0.6411\n",
      "Epoch 15/20, Loss: 0.6410\n",
      "Epoch 16/20, Loss: 0.6410\n",
      "Epoch 17/20, Loss: 0.6409\n",
      "Epoch 18/20, Loss: 0.6408\n",
      "Epoch 19/20, Loss: 0.6408\n",
      "Epoch 20/20, Loss: 0.6408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=6, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=6, out_features=4, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # Convert data to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train_normalized.values, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)  # Reshape for single output\n",
    "# X_test_tensor = torch.tensor(X_test_normalized.values, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# # Define a PyTorch dataset and dataloader\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # Define the Neural Network\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_size, 6),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(6, 4),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(4, 1),\n",
    "#             nn.Sigmoid()  # Output between 0 and 1 for binary classification\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# # Initialize the model, loss function, and optimizer\n",
    "# input_size = X_train_normalized.shape[1]\n",
    "# model = NeuralNetwork(input_size)\n",
    "\n",
    "# criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training Loop\n",
    "# num_epochs = 20\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         # Forward pass\n",
    "#         outputs = model(X_batch)\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "#         train_loss += loss.item()\n",
    "        \n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     train_loss /= len(train_loader)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "# # Evaluate the model on test data\n",
    "# model.eval()\n",
    "# # with torch.no_grad():\n",
    "#     y_pred = []\n",
    "#     y_true = []\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         outputs = model(X_batch)\n",
    "#         y_pred.extend(outputs.numpy())\n",
    "#         y_true.extend(y_batch.numpy())\n",
    "    \n",
    "#     # Convert predictions to binary outcomes\n",
    "#     y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# # Calculate metrics (e.g., accuracy)\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "# print(f\"Test Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model1\n",
      "Epoch 1/20, Loss: 0.6509\n",
      "Epoch 2/20, Loss: 0.6434\n",
      "Epoch 3/20, Loss: 0.6427\n",
      "Epoch 4/20, Loss: 0.6426\n",
      "Epoch 5/20, Loss: 0.6422\n",
      "Epoch 6/20, Loss: 0.6426\n",
      "Epoch 7/20, Loss: 0.6419\n",
      "Epoch 8/20, Loss: 0.6416\n",
      "Epoch 9/20, Loss: 0.6419\n",
      "Epoch 10/20, Loss: 0.6418\n",
      "Epoch 11/20, Loss: 0.6414\n",
      "Epoch 12/20, Loss: 0.6412\n",
      "Epoch 13/20, Loss: 0.6411\n",
      "Epoch 14/20, Loss: 0.6410\n",
      "Epoch 15/20, Loss: 0.6410\n",
      "Epoch 16/20, Loss: 0.6408\n",
      "Epoch 17/20, Loss: 0.6408\n",
      "Epoch 18/20, Loss: 0.6407\n",
      "Epoch 19/20, Loss: 0.6405\n",
      "Epoch 20/20, Loss: 0.6406\n",
      "Test Accuracy: 58.51%\n",
      "Training Model2\n",
      "Epoch 1/20, Loss: 0.6487\n",
      "Epoch 2/20, Loss: 0.6457\n",
      "Epoch 3/20, Loss: 0.6442\n",
      "Epoch 4/20, Loss: 0.6432\n",
      "Epoch 5/20, Loss: 0.6426\n",
      "Epoch 6/20, Loss: 0.6420\n",
      "Epoch 7/20, Loss: 0.6426\n",
      "Epoch 8/20, Loss: 0.6416\n",
      "Epoch 9/20, Loss: 0.6414\n",
      "Epoch 10/20, Loss: 0.6415\n",
      "Epoch 11/20, Loss: 0.6416\n",
      "Epoch 12/20, Loss: 0.6416\n",
      "Epoch 13/20, Loss: 0.6408\n",
      "Epoch 14/20, Loss: 0.6414\n",
      "Epoch 15/20, Loss: 0.6409\n",
      "Epoch 16/20, Loss: 0.6410\n",
      "Epoch 17/20, Loss: 0.6401\n",
      "Epoch 18/20, Loss: 0.6405\n",
      "Epoch 19/20, Loss: 0.6402\n",
      "Epoch 20/20, Loss: 0.6401\n",
      "Test Accuracy: 59.14%\n",
      "Training Model3\n",
      "Epoch 1/20, Loss: 0.6547\n",
      "Epoch 2/20, Loss: 0.6507\n",
      "Epoch 3/20, Loss: 0.6478\n",
      "Epoch 4/20, Loss: 0.6469\n",
      "Epoch 5/20, Loss: 0.6480\n",
      "Epoch 6/20, Loss: 0.6482\n",
      "Epoch 7/20, Loss: 0.6477\n",
      "Epoch 8/20, Loss: 0.6460\n",
      "Epoch 9/20, Loss: 0.6474\n",
      "Epoch 10/20, Loss: 0.6480\n",
      "Epoch 11/20, Loss: 0.6465\n",
      "Epoch 12/20, Loss: 0.6457\n",
      "Epoch 13/20, Loss: 0.6464\n",
      "Epoch 14/20, Loss: 0.6462\n",
      "Epoch 15/20, Loss: 0.6459\n",
      "Epoch 16/20, Loss: 0.6462\n",
      "Epoch 17/20, Loss: 0.6460\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_normalized.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)  # Reshape for single output\n",
    "X_test_tensor = torch.tensor(X_test_normalized.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define a PyTorch dataset and dataloader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "input_size = X_train_normalized.shape[1]\n",
    "\n",
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate_model(model, criterion, optimizer, num_epochs=20):\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            y_pred.extend(outputs.numpy())\n",
    "            y_true.extend(y_batch.numpy())\n",
    "        \n",
    "        # Convert predictions to binary outcomes\n",
    "        y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "    \n",
    "    # Calculate metrics (e.g., accuracy)\n",
    "    accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "    print(f\"Test Accuracy: {accuracy:.2%}\")\n",
    "    return accuracy\n",
    "\n",
    "# Define multiple models to test\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model1, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model2, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Model3(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model3, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class Model4(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model4, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Model5(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model5, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Model6(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model6, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Add additional models as needed...\n",
    "models = [\n",
    "    (\"Model1\", lambda: Model1(input_size)),\n",
    "    (\"Model2\", lambda: Model2(input_size)),\n",
    "    (\"Model3\", lambda: Model3(input_size)),\n",
    "    (\"Model4\", lambda: Model4(input_size)),\n",
    "    (\"Model5\", lambda: Model5(input_size)),\n",
    "    (\"Model6\", lambda: Model6(input_size)),\n",
    "]\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "for model_name, model_fn in models:\n",
    "    print(f\"Training {model_name}\")\n",
    "    model = model_fn()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    accuracy = train_and_evaluate_model(model, criterion, optimizer)\n",
    "    results[model_name] = accuracy\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "for model_name, accuracy in results.items():\n",
    "    print(f\"{model_name}: {accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
